#!/usr/bin/env python
"""
Count files and sizes on linux faster than anyone else.

This is created to identify large chunks of data and files hidden in subfolders on
large +10TB storage drives, where a normal 'find' or 'du' either takes forever or
does not identify the wanted hotspots.

It is Python, but uses libc to increase the handling of folders with too many files for other tools.


Main features:
 - Uses multiple processes to increase throughput when traversing networked filesystems.
 - Supports a progress option, which outputs a status for every S seconds. Very useful for folders with >100.000 files.
 - Filters output based on a minimum of filecount and/or filesize. Great for identifying hotspots with many >100.000 files.
 - Performs user separated counting in one go, thus reducing the required execution time.
 - Counts hard-linked files once.
 - Output format is very parser friendly.
 - Can perform counting at any depth level.

Author: Rune Moellegaard Friborg <runef@birc.au.dk>
"""


import getopt
import sys
import multiprocessing
import threading, time, Queue
import os
import os.path
import stat
import platform
import re
import numbers

##############################################################
####################### Configuration ########################

VERSION="0.72"
UPDATED="2014-10-07"
DEFAULT_NUM_WORKERS=8


##############################################################
####################### Human conversion #####################

units = { 'B': 2 ** 0,
          'K': 2 ** 10,
          'M': 2 ** 20,
          'G': 2 ** 30,
          'T': 2 ** 40,
          'P': 2 ** 50,
          'E': 2 ** 60,
          'Z': 2 ** 70,
          'Y': 2 ** 80 }

number_unit_pattern = r'^(?P<number>[0-9]*(\.[0-9]+)?)(?P<unit>[a-zA-Z]+)?$'
number_unit_matcher = re.compile(number_unit_pattern)

def supported_units():
  """Returns a list of valid unit suffixes.

  Unit suffixes are case-insensitive."""
  return units.keys()

def from_human(value, default_unit='M'):
  """Convert a file size in human format to bytes.

  Converts a string consisting of a floating point number and a unit suffix to
  bytes. If no unit suffix is found, default_unit is used. No whitespace is
  allowed in the string. Unit suffixes are case insensitive.

  E.g. to convert the string '512K' to bytes:

    >>> from_human('512K')
    524288
  """
  match = number_unit_matcher.match(value)
  if not match:
    raise ValueError('value must be a positive number followed by a valid unit')

  number, _, unit = match.groups()
  unit = unit or default_unit

  if not float(number) >= 0:
    raise ValueError('value must be a positive number followed by a valid unit')

  normalized_unit = unit.upper()
  if normalized_unit not in units.iterkeys():
    raise ValueError('unit is invalid')
  return int(float(number) * units[normalized_unit])

def to_human(value):
  """Convert a number of bytes to a human-readable string.

  Will convert any positive integer value to a human-readable string with a unit
  suffix such as K, M or G, depending on the size of the number. E.g.

    >>> to_human(1024)
    1.0K
    >>> to_human(524288)
    512.0K
    >>> to_human(78293719)
    74.7M
  """
  if not isinstance(value, numbers.Number):
    raise ValueError('first argument must be a positive integer')
  if not value >= 0:
    raise ValueError('first argument most be a positive integer')

  unit, multiplier = 'B', 1.0
  for curr_unit, curr_multiplier in units.iteritems():
    if value > curr_multiplier and \
       value < curr_multiplier * 1024:
      unit, multiplier = curr_unit, curr_multiplier
      break

  human_value = float(value) / multiplier
  return '{number:.1f}{unit}'.format(number=human_value, unit=unit)

##############################################################
####################### ctypes magic #########################

from ctypes import CDLL, c_char_p, c_uint, c_int, c_long, c_longlong, c_ushort, c_byte, c_char, Structure, POINTER
from ctypes.util import find_library

class c_dir(Structure):
  """Opaque type for directory entries, corresponds to struct DIR"""
  pass
c_dir_p = POINTER(c_dir)

class c_dirent(Structure):
  if platform.system() == 'Darwin':
    # Only used for testing
    _fields_ = [
      ('d_ino', c_long, 32), # inode number
      ('d_reclen', c_ushort, 16), # length of this record
      ('d_type', c_uint, 8), # type of file; not supported by all file system types
      ('d_namlen', c_ushort, 8), # type of file; not supported by all file system types
      ('d_name', c_char * 256) # filename
    ]
  else:
    # This should match the target systems expected for this application
    _fields_ = [
      ('d_ino', c_long), # inode number
      ('d_off', c_long), # offset to the next dirent
      ('d_reclen', c_ushort), # length of this record
      ('d_type', c_byte), # type of file; not supported by all file system types
      ('d_name', c_char * 4096) # filename
    ]
    
c_dirent_p = POINTER(c_dirent)

c_lib = CDLL(find_library("c"))
opendir = c_lib.opendir
opendir.argtypes = [c_char_p]
opendir.restype = c_dir_p

# FIXME Should probably use readdir_r here
readdir = c_lib.readdir
readdir.argtypes = [c_dir_p]
readdir.restype = c_dirent_p

closedir = c_lib.closedir
closedir.argtypes = [c_dir_p]
closedir.restype = c_int

D_TYPE_DIR = 4

def read_dir_entries(path):
  dir_p = opendir(".")
  try:
    while True:
      p = readdir(dir_p)
      if not p:
        break
      yield p.contents.d_name, p.contents.d_type
  finally:
    closedir(dir_p)

##############################################################
####################### GLOBAL VALUES ########################

# Checks for hardlinks
inode_list = set()

# Get all uids
import pwd
uidDict = {}
for p in pwd.getpwall():
  uidDict[p.pw_uid] = p.pw_name

# Unknown uids 
uidDict[None] = 'Unknown'

##############################################################
####################### Manager Thread #######################

class ManagerThread(threading.Thread):
  """
  Register the state of all workers and synchronise shutdown.
  Output progress info if enabled.
  """
  def __init__(self, seconds, q, workers):
    threading.Thread.__init__(self)
    self.seconds = seconds
    self.output = seconds > 0
    self.daemon = True
    self.workerJobsSubmitted = [0 for i in range(workers)]
    self.workerJobsDone = [0 for i in range(workers)]
    self.q = q
    self.template = '# {0:>5} files/s {1:>6} {2:>4} {3}\n'


  def run(self):
    previousCount = 0
    previousSize = 0
    newCount = 0
    newSize = 0
    previousTime = time.time()

    # Add one submitted job, for the initial root directory
    self.workerJobsSubmitted[0] += 1

    # This loop is runnning until the main thread exists
    while True:
      val = self.q.get()

      if len(val) == 2:
        # Process update (once for every dir traversed)
        submitted, pid = val
        self.workerJobsSubmitted[pid] += submitted
        self.workerJobsDone[pid] +=1
        
        # Check
        if sum(self.workerJobsSubmitted) == sum(self.workerJobsDone):
          # Start shutdown of workers
          for i in self.workerJobsDone:
            CountWorkerQueue.put((None, None, None, None))

          # Send done signal to main
          CountResultQueue.put((None, None, None, None, None))            

      else:
        # Progress update (unknown amount)
        cwd, count, size = val

        if self.output:
          newTime = time.time()
          newCount += count
          newSize += size
          if (newTime-previousTime > self.seconds):
            speed = (newCount-previousCount)/(newTime-previousTime)
            sys.stderr.write(self.template.format(int(speed), str(newCount), to_human(newSize), cwd))
            #sys.stderr.write("# "+str(int(speed))+" files/s "+str(newCount)+" "+to_human(newSize)+"B: "+cwd+"\n")
            sys.stderr.flush()
        
            previousTime = newTime
            previousSize = newSize
            previousCount = newCount
          

##############################################################
############ Stat Call Thread (on the fly) worker ############

class statFileWorkerThread(threading.Thread):
  """
  Short lived threads for performing file stats on large dirs.

  This thread is created to stat a specific list of files.
  """
  def __init__(self, cwd, q, files):
    threading.Thread.__init__(self)
    self.q = q
    self.cwd = cwd
    self.files = files
    self.daemon = True

  def run(self):
    groupstat(self.cwd, self.q, self.files)

# The stat function used either from the stat thread or directly
# from the count function, depending on the amount of files in 
# the dir in question
def groupstat(cwd, q, files):
    for name in files:
      abs_path = os.path.join(cwd, name)
      st = os.lstat(abs_path)
      q.put((name, st))


##############################################################
####################### Workers ##############################

# Allocate multiprocessing synchronised queues
StateQueue = multiprocessing.Queue()
CountWorkerQueue = multiprocessing.Queue()
CountResultQueue = multiprocessing.Queue()


class countWorkerProcess(multiprocessing.Process):
  def __init__(self, args, qIn, qOut, qState, pid):
    multiprocessing.Process.__init__(self)
    self.args = args
    self.qIn = qIn
    self.qOut = qOut
    self.qState = qState
    self.processindex = pid
    self.daemon = True

  def run(self):
    while True:
      path, name, parent_inode, inode = self.qIn.get()

      if path == None:
        # Quit
        break

      try:
        abs_path = os.path.join(path, name)
        os.chdir(abs_path)
        
        self.count(name, parent_inode, inode)

      except OSError, e:
        # Send state
        self.qState.put((0, self.processindex))

        # Handle error
        if e.errno == 13:
          sys.stderr.write('# access denied to directory ' + os.path.join(path, name) + '\n')
        elif e.errno == 2:
          sys.stderr.write('# could not access dir,file or file in dir ' + os.path.join(path, name) + '\n')
        else:
          raise e


  def count(self, dirname, parent_inode, inode):

    stateCount = 0
    stateSize = 0
    file_count = {}
    file_size = {}

    cwd = os.getcwd()
    # Thread Synchronised Queue
    q = Queue.Queue()

    # Get list of all files and dirs
    dirs = []  
    files = []
    for val in read_dir_entries(dirname):
      if val[0] == "." or val[0] == "..":
        continue

      if val[1] == D_TYPE_DIR:
        # Short cut.
        # If the file system supports the d_type flag, then we know this is dir.
        # If not supported, then the file stat code will detect if this is a dir.
        dirs.append(val[0])
        continue

      files.append(val[0])

    # Call stat on all files (non-dirs)
    l = len(files)
    c = 2
    if l > 1000:
      split = int(l/c)
      for i in xrange(c-1):
        t = statFileWorkerThread(cwd, q, files[i*split:(i+1)*split])
        t.start()
      t = statFileWorkerThread(cwd, q, files[(c-1)*split:])
      t.start()
    else:
      groupstat(cwd, q, files)

    # Collect stat info on all files (non-dirs)
    for i in xrange(len(files)):
      name, st = q.get()

      # Extra check to enable support for filesystems with no dirent.d_type
      if stat.S_ISDIR(st.st_mode):
        dirs.append(name)
        continue

      # If the inode wasn't visited before, we'll keep track of its
      # size. We save a lookup in inode_list by first asking if the
      # inode is linked to exactly once. If this is the case, it
      # cannot be in inode_list anyway.
      if st.st_nlink == 1 or st.st_ino not in inode_list:
        stateSize += st.st_size
        file_size[st.st_uid] = file_size.setdefault(st.st_uid, 0) + st.st_size

      # If inode is pointed to by more than one link, we keep track
      # of it so that we won't count the size of it more than once.
      if st.st_nlink > 1:
        inode_list.add(st.st_ino)

      # Always count up file_count.
      file_count[st.st_uid] = file_count.setdefault(st.st_uid, 0) + 1

      stateCount += 1
      # Send progress
      if (stateCount % 10000) == 0:
        self.qState.put((cwd, stateCount, stateSize))
        stateCount = 0
        stateSize = 0

    # Send state
    self.qState.put((len(dirs), self.processindex))
    
    # Call stat on all dirs
    if dirs:
      groupstat(cwd, q, dirs)

    # Collect stat info on all dirs
    for i in xrange(len(dirs)):
      name, st = q.get()

      # Add directory size allocation 
      stateSize += st.st_size
      file_size[st.st_uid] = file_size.setdefault(st.st_uid, 0) + st.st_size

      # Count directory as file entry
      file_count[st.st_uid] = file_count.setdefault(st.st_uid, 0) + 1
      stateCount += 1

      # Submit work to same or other multiprocess worker
      self.qIn.put((cwd, name, inode, st.st_ino))

    # Send progress
    self.qState.put((cwd, stateCount, stateSize))
      
    # Send result to main
    self.qOut.put((dirname, parent_inode, inode, file_count, file_size))

##############################################################
####################### Main #################################

def main(args):

  # Start manager thread
  t = ManagerThread(args.status, StateQueue, args.workers)
  t.start()

  # Start multiprocess workers
  for i in xrange(args.workers):
    p = countWorkerProcess(args, CountWorkerQueue, CountResultQueue, StateQueue, i)
    p.start()

  # Submit first dir to workers
  root_dir = args.directory
  CountWorkerQueue.put(('.', root_dir, 0, 1))


  # Handle the results from workers
  ALL = {}
  while True:
    name, parent_inode, inode, file_count, file_size = CountResultQueue.get()
    if name == None:
      # Done signal from manager
      break

    # Assemble data structure with all node dirs as keys and all leaf dirs as values
    if parent_inode in ALL:
      ALL[parent_inode].append((inode, name, file_count, file_size))
    else:
      ALL[parent_inode] = [(inode, name, file_count, file_size)]
    

  # Set output templates
  template_user = '{0:>15} {1:>15} {2:>15} {3}'
  template = '{0:>15} {1:>15} {2}'

  # Output header
  if not args.quiet:
    if args.user:
      print template_user.format('User', 'Files', 'Size', 'Path')
    else:
      print template.format('Files', 'Size', 'Path')
  
  
  # Perform depth-first collection
  def collect(n, p=[], d=0):
    #(1, '/home/runef/BACKUP/github/cfas', {6353: 4}, {6353: 36918})
    collect_file_count = n[2]
    collect_file_size = n[3]
    if n[0] in ALL:
      children = ALL[n[0]]

      for child in children:
        p.append(child[1])
        collect(child, p, d+1)
        p.pop()
        if not args.exclude_subdirs:
          # Include values for subdirs
          for uid in child[2]:
            collect_file_count[uid] = collect_file_count.setdefault(uid, 0) + child[2][uid]
          for uid in child[3]:
            collect_file_size[uid]  = collect_file_size.setdefault(uid, 0) + child[3][uid]

    if args.max_depth >= d:
      # Output
      if args.user:
        # Split output on user
        for uid in collect_file_count:
          if collect_file_count[uid] > args.file_limit or collect_file_size[uid] > args.size_limit:
            print template_user.format(uidDict.setdefault(uid, 'unknown('+str(uid)+')'),
                                       collect_file_count[uid], 
                                       (to_human(collect_file_size[uid]) if args.human_readable else collect_file_size[uid]),
                                       "/".join(p))
      else:
        # Summarise output
        sum_file_count = sum(collect_file_count.values())
        sum_file_size  = sum(collect_file_size.values())
        if sum_file_count >= args.file_limit or sum_file_size >= args.size_limit:
          print template.format(sum_file_count, 
                                (to_human(sum_file_size) if args.human_readable else sum_file_size),
                                "/".join(p))
  try:
    n = ALL[0][0]
    collect(n, [n[1]])
  except KeyError:
    # This is probably because the root dir could not be accessed
    pass
    
  

##############################################################
######################### Help ###############################

def usage():
    print("""cfas version """+VERSION+""" by Rune M. Friborg (updated """+UPDATED+""")
Usage:
  cfas [-h] [--directory DIRECTORY] [--file-limit N] [--size-limit K]
            [--exclude-subdirs] [--quiet] [--user] [--human-readable]
            [--status S]

Optional arguments:
  -h, --help            show this help message and exit
  --directory DIRECTORY, -d DIRECTORY
                        the directory to walk
  --max-depth d, -D d   only output results for folder level up to d.
                        -D 0 is similar to 'du -s'

  --file-limit N, -n N  output directory if it contains at least N files
  --size-limit K, -k K  output directory if the total size is at least K bytes.
                          Size suffixes such as P,T,G,M,K may be specified.
  
  --exclude-subdirs     all directories are counted separately

  --quiet, -q           do not output header (usefull for output parsing)
  --user, -u            add user column and split counts between users

  --human-readable, -H  output numbers in human readable format

  --status S, -s S      output status to stderr for every S second

  --workers P, -w P     number of workers to traverse dirs (default: """+str(DEFAULT_NUM_WORKERS)+""")
""")

  
class ArgContainer():
  def __init__(self):
    self.directory      = os.getcwd()
    self.file_limit     = 0
    self.size_limit     = 0
    self.exclude_subdirs= False
    self.max_depth      = 1000
    self.quiet          = False
    self.human_readable = False
    self.user           = False
    self.status         = 0
    self.workers        = DEFAULT_NUM_WORKERS



if __name__ == '__main__':
  #args = parser.parse_args()
  #args.file_limit = 
  #args.size_limit = int(from_human(args.size_limit, default_unit='B'))

  try:
    opts, _ = getopt.getopt(sys.argv[1:], "hd:D:n:k:qHup:s:w:", ["help", "directory=", "max-depth=", "file-limit=", "size-limit=", "exclude-subdirs", "quiet", "human-readable", "user", "progress=", "status=", "workers="])
  except getopt.GetoptError, err:
    # print help information and exit:
    print str(err) # will print something like "option -a not recognized"
    sys.exit(2)

  args = ArgContainer()
  for o, a in opts:
    if a and a[0] == "=":
      a = a[1:]

    if o in ("-d", "--directory"):
      args.directory = a
    elif o in ("-D", "--max-depth"):
      args.max_depth = int(a)
    elif o in ("-n", "--file-limit"):
      args.file_limit = int(from_human(a, default_unit='B'))
    elif o in ("-k", "--size-limit"):
      args.size_limit = int(from_human(a, default_unit='B'))
    elif o == "--exclude-subdirs":
      args.exclude_subdirs = True
    elif o in ("-q", "--quiet"):
      args.quiet = True
    elif o in ("-H", "--human-readable"):
      args.human_readable = True
    elif o in ("-u", "--user"):
      args.user = True
    elif o in ("-p", "--progress"):
      #ignore
      pass
    elif o in ("-s", "--status"):
      args.status = int(a)    
    elif o in ("-w", "--workers"):
      args.workers = max(1,int(a))
    elif o in ("-h", "--help"):
      usage()
      sys.exit()
    else:
      assert False, "unhandled option"

  if args.file_limit == 0 and args.size_limit > 0:
    args.file_limit = 10**18
  elif args.size_limit == 0 and args.file_limit > 0:
    args.size_limit = 10**18

  try:
    main(args)
  except KeyboardInterrupt:
    sys.exit(1)

