#!/usr/bin/env python2.7
"""
Count files and sizes on linux faster than anyone else.

This is created to identify large chunks of data and files hidden in subfolders on
large +10TB storage drives, where a normal 'find' or 'du' either takes forever or
does not identify the wanted hotspots.

It is Python, but uses libc to increase the handling of folders with too many files for other tools.


Main features:
 - Uses multiple processes to increase throughput when traversing networked filesystems.
 - Supports a progress option, which outputs a status for every S seconds. Very useful for folders with >100.000 files.
 - Filters output based on a minimum of filecount and/or filesize. Great for identifying hotspots with many >100.000 files.
 - Performs user separated counting in one go, thus reducing the required execution time.
 - Counts hard-linked files once.
 - Output format is very parser friendly.
 - Can perform counting at any depth level.

Author: Rune Moellegaard Friborg <runef@birc.au.dk>
"""


import getopt
import sys
import multiprocessing
import threading, time, Queue
import os
import os.path
import stat
import platform
import re
import numbers

##############################################################
####################### Configuration ########################

VERSION="0.77"
UPDATED="2019-02-26"
DEFAULT_NUM_WORKERS=8


##############################################################
####################### Human conversion #####################

units = { 'B': 2 ** 0,
          'K': 2 ** 10,
          'M': 2 ** 20,
          'G': 2 ** 30,
          'T': 2 ** 40,
          'P': 2 ** 50,
          'E': 2 ** 60,
          'Z': 2 ** 70,
          'Y': 2 ** 80 }

number_unit_pattern = r'^(?P<number>[0-9]*(\.[0-9]+)?)(?P<unit>[a-zA-Z]+)?$'
number_unit_matcher = re.compile(number_unit_pattern)

def supported_units():
  """Returns a list of valid unit suffixes.

  Unit suffixes are case-insensitive."""
  return units.keys()

def from_human(value, default_unit='M'):
  """Convert a file size in human format to bytes.

  Converts a string consisting of a floating point number and a unit suffix to
  bytes. If no unit suffix is found, default_unit is used. No whitespace is
  allowed in the string. Unit suffixes are case insensitive.

  E.g. to convert the string '512K' to bytes:

    >>> from_human('512K')
    524288
  """
  match = number_unit_matcher.match(value)
  if not match:
    raise ValueError('value must be a positive number followed by a valid unit')

  number, _, unit = match.groups()
  unit = unit or default_unit

  if not float(number) >= 0:
    raise ValueError('value must be a positive number followed by a valid unit')

  normalized_unit = unit.upper()
  if normalized_unit not in units.iterkeys():
    raise ValueError('unit is invalid')
  return int(float(number) * units[normalized_unit])

def to_human(value):
  """Convert a number of bytes to a human-readable string.

  Will convert any positive integer value to a human-readable string with a unit
  suffix such as K, M or G, depending on the size of the number. E.g.

    >>> to_human(1024)
    1.0K
    >>> to_human(524288)
    512.0K
    >>> to_human(78293719)
    74.7M
  """
  if not isinstance(value, numbers.Number):
    raise ValueError('first argument must be a positive integer')
  if not value >= 0:
    raise ValueError('first argument most be a positive integer')

  unit, multiplier = 'B', 1.0
  for curr_unit, curr_multiplier in units.iteritems():
    if value > curr_multiplier and \
       value < curr_multiplier * 1024:
      unit, multiplier = curr_unit, curr_multiplier
      break

  human_value = float(value) / multiplier
  return '{number:.1f}{unit}'.format(number=human_value, unit=unit)

##############################################################
####################### ctypes magic #########################

from ctypes import CDLL, c_char_p, c_uint, c_int, c_long, c_longlong, c_ushort, c_byte, c_char, Structure, POINTER
from ctypes.util import find_library

class c_dir(Structure):
  """Opaque type for directory entries, corresponds to struct DIR"""
  pass
c_dir_p = POINTER(c_dir)

class c_dirent(Structure):
  if platform.system() == 'Darwin':
    # Only used for testing
    _fields_ = [
      ('d_ino', c_long, 32), # inode number
      ('d_reclen', c_ushort, 16), # length of this record
      ('d_type', c_uint, 8), # type of file; not supported by all file system types
      ('d_namlen', c_ushort, 8), # type of file; not supported by all file system types
      ('d_name', c_char * 256) # filename
    ]
  else:
    # This should match the target systems expected for this application
    _fields_ = [
      ('d_ino', c_long), # inode number
      ('d_off', c_long), # offset to the next dirent
      ('d_reclen', c_ushort), # length of this record
      ('d_type', c_byte), # type of file; not supported by all file system types
      ('d_name', c_char * 4096) # filename
    ]
    
c_dirent_p = POINTER(c_dirent)

c_lib = CDLL(find_library("c"))
opendir = c_lib.opendir
opendir.argtypes = [c_char_p]
opendir.restype = c_dir_p

# FIXME Should probably use readdir_r here
readdir = c_lib.readdir
readdir.argtypes = [c_dir_p]
readdir.restype = c_dirent_p

closedir = c_lib.closedir
closedir.argtypes = [c_dir_p]
closedir.restype = c_int

D_TYPE_DIR = 4

def read_dir_entries(path):
  dir_p = opendir(".")
  try:
    while True:
      p = readdir(dir_p)
      if not p:
        break
      yield p.contents.d_name, p.contents.d_type
  finally:
    closedir(dir_p)

##############################################################
####################### GLOBAL VALUES ########################

# Checks for hardlinks
inode_list = set()

# Get all uids
import pwd
uidDict = {}
for p in pwd.getpwall():
  uidDict[p.pw_uid] = p.pw_name

# Unknown uids 
uidDict[None] = 'Unknown'

##############################################################
####################### Messages #############################

# Sent on CountResultQueue
class ManagerDoneMsg():
  def __init__(self, total_work):
     self.total_work = total_work

# Sent on CountResultQueue
class ResultMsg():
  def __init__(self, name, parent_inode, inode, file_count, file_size):
     self.name = name
     self.parent_inode = parent_inode
     self.inode = inode
     self.file_count = file_count
     self.file_size = file_size

# Sent on CountResultQueue
class ErrResultMsg():
  def __init__(self):
    pass

# Sent on CountWorkerQueue
class WorkerShutdownMsg():
  def __init__(self):
    pass

# Send on CountWorkerQueue
class WorkMsg():
  def __init__(self, path, name, parent_inode, inode):
    self.path = path
    self.name = name
    self.parent_inode = parent_inode
    self.inode = inode

# Sent on StateQueue
class StateMsg():
  def __init__(self, cwd, count, size):
    self.cwd = cwd
    self.count = count
    self.size = size

class DirStateMsg():
  def __init__(self, submitted, pid):
    self.submitted = submitted
    self.pid = pid

##############################################################
####################### Manager Thread #######################

class ManagerThread(threading.Thread):
  """
  Register the state of all workers and synchronise shutdown.
  Output progress info if enabled.
  """
  def __init__(self, seconds, q, workers):
    threading.Thread.__init__(self)
    self.seconds = seconds
    self.output = seconds >= 0
    self.daemon = True
    self.workerJobsSubmitted = [0 for i in range(workers)]
    self.workerJobsDone = [0 for i in range(workers)]
    self.q = q
    self.template = '# {0:>5} files/s {1:>6} {2:>4} {3}\n'


  def run(self):
    previousCount = 0
    previousSize = 0
    newCount = 0
    newSize = 0
    previousTime = time.time()

    # This loop is runnning until the main thread exists
    while True:
      msg = self.q.get()

      if isinstance(msg, DirStateMsg):
        # Process update (once for every dir traversed)
        self.workerJobsSubmitted[msg.pid] += msg.submitted
        self.workerJobsDone[msg.pid] +=1
        
        # Check
        if sum(self.workerJobsSubmitted) == sum(self.workerJobsDone):
          # Start shutdown of workers
          for i in self.workerJobsDone:
            CountWorkerQueue.put(WorkerShutdownMsg())

          # Send done signal to main
          CountResultQueue.put(ManagerDoneMsg(sum(self.workerJobsDone)-1))            
          break

      else:
        # Progress update (unknown amount)
        if self.output:
          newTime = time.time()
          newCount += msg.count
          newSize += msg.size
          if (newTime-previousTime > self.seconds):
            speed = (newCount-previousCount)/(newTime-previousTime)
            sys.stderr.write(self.template.format(int(speed), str(newCount), to_human(newSize), msg.cwd))
            sys.stderr.flush()
        
            previousTime = newTime
            previousSize = newSize
            previousCount = newCount
          

##############################################################
############ Stat Call Thread (on the fly) worker ############

class statFileWorkerThread(threading.Thread):
  """
  Short lived threads for performing file stats on large dirs.

  This thread is created to stat a specific list of files.
  """
  def __init__(self, cwd, q, files):
    threading.Thread.__init__(self)
    self.q = q
    self.cwd = cwd
    self.files = files
    self.daemon = True

  def run(self):
    groupstat(self.cwd, self.q, self.files)

# The stat function used either from the stat thread or directly
# from the count function, depending on the amount of files in 
# the dir in question
def groupstat(cwd, q, files):
    for name in files:
      abs_path = os.path.join(cwd, name)
      st = os.lstat(abs_path)
      q.put((name, st))


##############################################################
####################### Workers ##############################

# Allocate multiprocessing synchronised queues
StateQueue = multiprocessing.Queue()
CountWorkerQueue = multiprocessing.Queue()
CountResultQueue = multiprocessing.Queue()


class countWorkerProcess(multiprocessing.Process):
  def __init__(self, args, qIn, qOut, qState, pid):
    multiprocessing.Process.__init__(self)
    self.args = args
    self.qIn = qIn
    self.qOut = qOut
    self.qState = qState
    self.processindex = pid
    self.daemon = True

  def run(self):
    while True:
      msg = self.qIn.get()

      if isinstance(msg, WorkerShutdownMsg):
        # Quit
        break

      try:
        abs_path = os.path.join(msg.path, msg.name)

        os.chdir(abs_path)
        
        self.count(msg.name, msg.parent_inode, msg.inode)

      except OSError, e:
        # Send state
        self.qState.put(DirStateMsg(0, self.processindex))

        self.qOut.put(ErrResultMsg())

        # Handle error
        if e.errno == 13:
          sys.stderr.write('# access denied to directory ' + os.path.join(msg.path, msg.name) + '\n')
        elif e.errno == 2:
          sys.stderr.write('# could not access dir,file or file in dir ' + os.path.join(msg.path, msg.name) + '\n')
        else:
          raise e


  def count(self, dirname, parent_inode, inode):

    stateCount = 0
    stateSize = 0
    file_count = {}
    file_size = {}

    cwd = os.getcwd()
    # Thread Synchronised Queue
    q = Queue.Queue()

    # Get list of all files and dirs
    dirs = []  
    files = []
    for val in read_dir_entries(dirname):
      if val[0] == "." or val[0] == "..":
        continue

      # Pattern mathing for excluding files
      if self.args.exclude:
        abs_path = os.path.join(cwd, val[0])
        if self.args.exclude.match(abs_path) != None:
          # Matched! skip path
          continue

      if val[1] == D_TYPE_DIR:
        # Short cut.
        # If the file system supports the d_type flag, then we know this is dir.
        # If not supported, then the file stat code will detect if this is a dir.
        dirs.append(val[0])
        continue

      files.append(val[0])

    # Call stat on all files (non-dirs)
    l = len(files)
    c = 2
    if l > 1000:
      split = int(l/c)
      for i in xrange(c-1):
        t = statFileWorkerThread(cwd, q, files[i*split:(i+1)*split])
        t.start()
      t = statFileWorkerThread(cwd, q, files[(c-1)*split:])
      t.start()
    else:
      groupstat(cwd, q, files)

    # Collect stat info on all files (non-dirs)
    for i in xrange(len(files)):
      name, st = q.get()

      # Extra check to enable support for filesystems with no dirent.d_type
      if stat.S_ISDIR(st.st_mode):
        dirs.append(name)
        continue

      # Pattern mathing for including files
      if self.args.include:
        abs_path = os.path.join(cwd, name)
        if self.args.include.match(abs_path) == None:
          # No match! skip path
          continue

      # TODO! This does not work for cases where there is more than one
      # worker!
      # This should be an optional setting.
      #
      # If the inode wasn't visited before, we'll keep track of its
      # size. We save a lookup in inode_list by first asking if the
      # inode is linked to exactly once. If this is the case, it
      # cannot be in inode_list anyway.
      if st.st_nlink == 1 or st.st_ino not in inode_list:
        stateSize += st.st_size
        file_size[st.st_uid] = file_size.setdefault(st.st_uid, 0) + st.st_size

      # If inode is pointed to by more than one link, we keep track
      # of it so that we won't count the size of it more than once.
      if st.st_nlink > 1:
        inode_list.add(st.st_ino)

      # Always count up file_count.
      file_count[st.st_uid] = file_count.setdefault(st.st_uid, 0) + 1

      stateCount += 1
      # Send progress
      if (stateCount % 10000) == 0:
        self.qState.put(StateMsg(cwd, stateCount, stateSize))
        stateCount = 0
        stateSize = 0

    # Send state
    self.qState.put(DirStateMsg(len(dirs), self.processindex))
    
    # Call stat on all dirs
    if dirs:
      groupstat(cwd, q, dirs)

    # Collect stat info on all dirs
    for i in xrange(len(dirs)):
      name, st = q.get()

      # Pattern mathing for including folders
      if self.args.include:
        abs_path = os.path.join(cwd, name)
        if self.args.include.match(abs_path) == None:
          # No match! skip path
          file_size[st.st_uid] = file_size.setdefault(st.st_uid, 0)
          file_count[st.st_uid] = file_count.setdefault(st.st_uid, 0)
          self.qIn.put(WorkMsg(cwd, name, inode, st.st_ino))
          continue

      # Add directory size allocation 
      stateSize += st.st_size
      file_size[st.st_uid] = file_size.setdefault(st.st_uid, 0) + st.st_size

      # Count directory as file entry
      file_count[st.st_uid] = file_count.setdefault(st.st_uid, 0) + 1
      stateCount += 1

      # Submit work to same or other multiprocess worker
      self.qIn.put(WorkMsg(cwd, name, inode, st.st_ino))

    # Send progress
    self.qState.put(StateMsg(cwd, stateCount, stateSize))
     
    # Send result to main
    self.qOut.put(ResultMsg(dirname, parent_inode, inode, file_count, file_size))


##############################################################
####################### Main #################################

def main(args):

  # Start manager thread
  t = ManagerThread(args.status, StateQueue, args.workers)
  t.start()

  # Start multiprocess workers
  for i in xrange(args.workers):
    p = countWorkerProcess(args, CountWorkerQueue, CountResultQueue, StateQueue, i)
    p.start()


  search_dirs = []
  try:
    # Get inodes of every dir
    root_dirs=[]
    cwd = os.getcwd()
    for d in args.dirlist:
      abs_path = os.path.join(cwd, d)
      st = os.lstat(abs_path)
      root_dirs.append((os.path.realpath(d), d, st.st_ino))

    # Filter dirlist to exclude all subdirs
    search_dirs = root_dirs[:]
    search_dirs.sort()
    i = 0
    # Removes subdirs
    while (i < (len(search_dirs)-1)):
      if search_dirs[i][0] == search_dirs[i+1][0][0:len(search_dirs[i][0])]:
        search_dirs.pop(i+1)
      else:        
        i += 1

  except OSError, e:
    # Handle error
    sys.stderr.write(e.strerror + ": '" + e.filename + "'\n")
    sys.exit(1)

  # Submit size of dirlist
  StateQueue.put(DirStateMsg(1+len(search_dirs), 0))
  
  # Submit dirlist to workers
  for realpath, dirname, inode in search_dirs:
    CountWorkerQueue.put(WorkMsg('.', realpath, 0, inode))


  # Handle the results from workers
  ALL = {}
  TREE = {}
  total = -1
  received = 0
  while total == -1 or received < total:
    msg = CountResultQueue.get()
    if isinstance(msg, ManagerDoneMsg):
      # Done signal from manager
      total = msg.total_work
    elif isinstance(msg, ResultMsg):
      received += 1
      # Assemble tree data structure to resemble the inode relations (only directories)
      if msg.parent_inode in TREE:
        TREE[msg.parent_inode].append(msg.inode)
      else:
        TREE[msg.parent_inode] = [msg.inode]

      # Store each inode entry (only directories)
      ALL[msg.inode] = (msg.name, msg.file_count, msg.file_size, [False])
    else:
      # ErrResultMsg
      received += 1


  # Set output templates
  template_user = '{0:>15} {1:>15} {2:>15} {3}'
  template = '{0:>15} {1:>15} {2}'

  # Output header
  if not args.quiet:
    if args.user:
      print template_user.format('User', 'Files', 'Size', 'Path')
    else:
      print template.format('Files', 'Size', 'Path')


  # Perform depth-first collection
  depth_first_stack=[]
  def perform_collect(inode, n, p=[], d=0):
      depth_first_stack = [(inode, n, p, d)]
      reverse_stack = []
      while (depth_first_stack):
          build_stack(depth_first_stack, reverse_stack)
      while (reverse_stack):
          collect(reverse_stack)

  def build_stack(depth_first_stack, reverse_stack):
    node = depth_first_stack.pop()
    reverse_stack.append(node)
    inode, n, p, d = node

    if inode in TREE:
      children = TREE[inode]
      for childinode in children[::-1]:
        child = ALL[childinode]
        depth_first_stack.append((childinode, child, p + [child[0]], d+1))


  def collect(reverse_stack):
    inode, n, p, d = reverse_stack.pop()

    #n = ('/home/runef/BACKUP/github/cfas', {6353: 4}, {6353: 36918}, counted?)
    collect_file_count = n[1]
    collect_file_size = n[2]

    if inode in TREE:
      children = TREE[inode]

      for childinode in children:
        child = ALL[childinode]

        if not args.exclude_subdirs and not child[3][0]:
          child[3][0] = True
          # Include values for subdirs
          for uid in child[1]:
            collect_file_count[uid] = collect_file_count.setdefault(uid, 0) + child[1][uid]
          for uid in child[2]:
            collect_file_size[uid]  = collect_file_size.setdefault(uid, 0) + child[2][uid]
 
    # Summarise output
    sum_file_count = sum(collect_file_count.values())
    sum_file_size  = sum(collect_file_size.values())

    if sum_file_count == 0 and sum_file_size == 0:
      # Pattern mathing for excluding
      if args.exclude:
        abs_path = "/".join(p)
        if args.exclude.match(abs_path) != None:
          # Match!
          return

      # Pattern mathing for including
      if args.include:
        abs_path = "/".join(p)
        if args.include.match(abs_path) == None:
          # No match!
          return


    if args.max_depth >= d:
      # Output
      if args.user:
        # Split output on user
        for uid in collect_file_count:
          if collect_file_count[uid] > args.file_limit or collect_file_size[uid] > args.size_limit:
            print template_user.format(uidDict.setdefault(uid, 'unknown('+str(uid)+')'),
                                       collect_file_count[uid], 
                                       (to_human(collect_file_size[uid]) if args.human_readable else collect_file_size[uid]),
                                       "/".join(p))
      else:
        if sum_file_count >= args.file_limit or sum_file_size >= args.size_limit:
          print template.format(sum_file_count, 
                                (to_human(sum_file_size) if args.human_readable else sum_file_size),
                                "/".join(p))
  try:
    #for realpath, dirname, inode in root_dirs:
    #  collect(ALL[inode]

    for realpath, dirname, inode in root_dirs:
      root_entry = ALL[inode]
      perform_collect(inode, root_entry, [dirname])
  except KeyError:
    # This is probably because the root dir could not be accessed
    pass
    
  

##############################################################
######################### Help ###############################

def usage():
    print("""cfas version """+VERSION+""" by Rune M. Friborg (updated """+UPDATED+""")
Usage:
  cfas [--file-limit N] [--size-limit K] [--max-depth D]
       [--exclude PATTERN] [--include PATTERN]
       [--exclude-subdirs] [--quiet] [--user] [--human-readable]
       [--status S] [DIRECTORIES TO WALK..]

Optional arguments:
  --help                show this help message and exit
  --max-depth D, -d D   only output results for folder level up to d.
                        -d 0 is similar to 'du -s'

  --file-limit N, -n N  output directory if it contains at least N files
  --size-limit K, -k K  output directory if the total size is at least K bytes.
                          Size suffixes such as P,T,G,M,K may be specified.

  --exclude PATTERN     exclude paths/files matching PATTERN
  --include PATTERN     don't exclude paths/files matching PATTERN

  --exclude-subdirs     all directories are counted separately

  --quiet, -q           do not output header (usefull for output parsing)
  --user, -u            add user column and split counts between users

  --human-readable, -h  output numbers in human readable format

  --status S, -s S      output status to stderr for every S second

  --workers P, -w P     number of workers to traverse dirs (default: """+str(DEFAULT_NUM_WORKERS)+""")
""")

  
class ArgContainer():
  def __init__(self):
    self.dirlist        = []
    self.file_limit     = 0
    self.size_limit     = 0
    self.exclude        = False
    self.include        = False
    self.exclude_subdirs= False
    self.max_depth      = 1000
    self.quiet          = False
    self.human_readable = False
    self.user           = False
    self.status         = -1
    self.workers        = DEFAULT_NUM_WORKERS


if __name__ == '__main__':
  #args = parser.parse_args()
  #args.file_limit = 
  #args.size_limit = int(from_human(args.size_limit, default_unit='B'))

  try:
    opts, dirs = getopt.getopt(sys.argv[1:], "d:n:k:qhup:s:w:", ["help", "directory=", "max-depth=", "file-limit=", "size-limit=", "exclude=", "include=", "exclude-subdirs", "quiet", "human-readable", "user", "progress=", "status=", "workers="])
  except getopt.GetoptError, err:
    # print help information and exit:
    print str(err) # will print something like "option -a not recognized"
    sys.exit(2)

  args = ArgContainer()
  for o, a in opts:
    if a and a[0] == "=":
      a = a[1:]

    if o == "--directory":
      #args.directory = a
      sys.stderr.write("Deprecated -d, --directory. Add directories to search without -d, --directory\n")
      sys.exit(1)
    elif o in ("-d", "--max-depth"):
      args.max_depth = int(a)
    elif o in ("-n", "--file-limit"):
      args.file_limit = int(from_human(a, default_unit='B'))
    elif o in ("-k", "--size-limit"):
      args.size_limit = int(from_human(a, default_unit='B'))
    elif o == "--exclude":
      try:
        exclude_matcher = re.compile(a + "$")
      except Exception, e:
        sys.stderr.write("Could not parse --exclude pattern '" + str(a) + "'\n")
        sys.stderr.write("Reason: " + e.message + "\n")
        sys.exit(1)
      args.exclude = exclude_matcher
    elif o == "--include":
      try:
        include_matcher = re.compile(a + "$")
      except Exception, e:
        sys.stderr.write("Could not parse --include pattern '" + str(a) + "'\n")
        sys.stderr.write("Reason: " + e.message + "\n")
        sys.exit(1)
      args.include = include_matcher
    elif o == "--exclude-subdirs":
      args.exclude_subdirs = True
    elif o in ("-q", "--quiet"):
      args.quiet = True
    elif o in ("-h", "--human-readable"):
      args.human_readable = True
    elif o in ("-u", "--user"):
      args.user = True
    elif o in ("-p", "--progress"):
      #ignore
      pass
    elif o in ("-s", "--status"):
      args.status = int(a)    
    elif o in ("-w", "--workers"):
      args.workers = max(1,int(a))
    elif o == "--help":
      usage()
      sys.exit()
    else:
      assert False, "unhandled option"

  if dirs:
    args.dirlist.extend(dirs)

  if not args.dirlist:
    # Add current dir
    args.dirlist.append(os.getcwd())

  if args.file_limit == 0 and args.size_limit > 0:
    args.file_limit = 10**18
  elif args.size_limit == 0 and args.file_limit > 0:
    args.size_limit = 10**18

  try:
    main(args)
  except KeyboardInterrupt:
    sys.exit(1)

